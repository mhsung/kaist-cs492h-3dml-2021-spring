{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Minhyuk Sung (mhsung@kaist.ac.kr)\n",
    "\n",
    "from pointnet import PointNetCls\n",
    "\n",
    "import easydict\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check whether GPU is available.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters.\n",
    "args = easydict.EasyDict({\n",
    "    'train': False,\n",
    "    'batch_size': 32,       # input batch size\n",
    "    'n_epochs': 50,         # number of epochs\n",
    "    'n_workers': 4,         # number of data loading workers\n",
    "    'learning_rate': 0.001, # learning rate\n",
    "    'beta1': 0.9,           # beta 1\n",
    "    'beta2': 0.999,         # beta 2\n",
    "    'step_size': 20,        # step size\n",
    "    'gamma': 0.5,           # gamma\n",
    "    'in_data_file': 'data/ModelNet/modelnet_classification.h5', # data directory\n",
    "    'model': 'outputs/model_50.pth',                            # model path\n",
    "    'out_dir': 'outputs'    # output directory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 'Dataset' class.\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, point_clouds, class_ids):\n",
    "        # point_clouds: (N, 3)\n",
    "        self.point_clouds = torch.from_numpy(point_clouds).float()\n",
    "        # class_ids: (N)\n",
    "        self.class_ids = torch.from_numpy(class_ids).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.shape(self.point_clouds)[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.point_clouds[idx], self.class_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and create dataloaders.\n",
    "def create_datasets_and_dataloaders():\n",
    "    assert(os.path.exists(args.in_data_file))\n",
    "    f = h5py.File(args.in_data_file, 'r')\n",
    "\n",
    "    train_data = Dataset(f['train_point_clouds'][:], f['train_class_ids'][:])\n",
    "    test_data = Dataset(f['test_point_clouds'][:], f['test_class_ids'][:])\n",
    "\n",
    "    n_classes = np.amax(f['train_class_ids']) + 1\n",
    "    print('# classes: {:d}'.format(n_classes))\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=args.train,\n",
    "        num_workers=int(args.n_workers))\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=args.train,\n",
    "        num_workers=int(args.n_workers))\n",
    "\n",
    "    return train_data, train_dataloader, test_data, test_dataloader, n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cross entropy loss function.\n",
    "def compute_loss(points, gt_classes, pred_class_logits):\n",
    "    # points: (batch_size, n_points, dim_input)\n",
    "    # gt_classes: (batch_size)\n",
    "    # pred_class_logits: (batch_size, n_classes)\n",
    "    loss = F.cross_entropy(input=pred_class_logits, target=gt_classes)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the accuracy function.\n",
    "def compute_accuracy(points, gt_classes, pred_class_logits):\n",
    "    # points: (batch_size, n_points, dim_input)\n",
    "    # gt_classes: (batch_size)\n",
    "    # pred_class_logits: (batch_size, n_classes)\n",
    "    pred_classes = pred_class_logits.max(1)[1]\n",
    "    acc = float(pred_classes.eq(gt_classes).sum()) / gt_classes.size()[0] * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one-step training function.\n",
    "def run_train(data, net, optimizer, writer=None):\n",
    "    # Parse data.\n",
    "    points, gt_classes = data\n",
    "    points = points.cuda()\n",
    "    gt_classes = gt_classes.cuda().squeeze()\n",
    "    # points: (batch_size, n_points, dim_input)\n",
    "    # gt_classes: (batch_size)\n",
    "\n",
    "    # Reset gradients.\n",
    "    # https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html#zero-the-gradients-while-training-the-network\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Predict.\n",
    "    pred_class_logits = net.train()(points)\n",
    "\n",
    "    # Compute the loss.\n",
    "    loss = compute_loss(points, gt_classes, pred_class_logits)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Compute the accuracy.\n",
    "        acc = compute_accuracy(points, gt_classes, pred_class_logits)\n",
    "\n",
    "    # Backprop.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one-step evaluation function.\n",
    "def run_eval(data, net, optimizer, writer=None):\n",
    "    # Parse data.\n",
    "    points, gt_classes = data\n",
    "    points = points.cuda()\n",
    "    gt_classes = gt_classes.cuda().squeeze()\n",
    "    # points: (batch_size, n_points, dim_input)\n",
    "    # gt_classes: (batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Predict.\n",
    "        pred_class_logits = net.eval()(points)\n",
    "\n",
    "        # Compute the loss.\n",
    "        loss = compute_loss(points, gt_classes, pred_class_logits)\n",
    "\n",
    "        # Compute the accuracy.\n",
    "        acc = compute_accuracy(points, gt_classes, pred_class_logits)\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one-epoch training/evaluation function.\n",
    "def run_epoch(dataset, dataloader, train, epoch=None, writer=None):\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    n_data = len(dataset)\n",
    "\n",
    "    # Create a progress bar.\n",
    "    pbar = tqdm(total=n_data, leave=False)\n",
    "\n",
    "    mode = 'Train' if train else 'Test'\n",
    "    epoch_str = '' if epoch is None else '[Epoch {}/{}]'.format(\n",
    "            str(epoch).zfill(len(str(args.n_epochs))), args.n_epochs)\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # Run one step.\n",
    "        loss, acc = run_train(data, net, optimizer, writer) if train else \\\n",
    "                run_eval(data, net, optimizer, writer)\n",
    "\n",
    "        if train and writer is not None:\n",
    "            # Write results if training.\n",
    "            assert(epoch is not None)\n",
    "            step = epoch * len(dataloader) + i\n",
    "            writer.add_scalar('Loss/Train', loss, step)\n",
    "            writer.add_scalar('Accuracy/Train', acc, step)\n",
    "\n",
    "        batch_size = list(data[0].size())[0]\n",
    "        total_loss += (loss * batch_size)\n",
    "        total_acc += (acc * batch_size)\n",
    "\n",
    "        pbar.set_description('{} {} Loss: {:f}, Acc : {:.2f}%'.format(\n",
    "            epoch_str, mode, loss, acc))\n",
    "        pbar.update(batch_size)\n",
    "\n",
    "    pbar.close()\n",
    "    mean_loss = total_loss / float(n_data)\n",
    "    mean_acc = total_acc / float(n_data)\n",
    "    return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one-epoch function for both training and evaluation.\n",
    "def run_epoch_train_and_test(\n",
    "    train_dataset, train_dataloader, test_dataset, test_dataloader, epoch=None,\n",
    "        writer=None):\n",
    "    train_loss, train_acc = run_epoch(\n",
    "        train_dataset, train_dataloader, train=args.train, epoch=epoch,\n",
    "        writer=writer)\n",
    "    test_loss, test_acc = run_epoch(\n",
    "        test_dataset, test_dataloader, train=False, epoch=epoch, writer=None)\n",
    "\n",
    "    if writer is not None:\n",
    "        # Write test results.\n",
    "        assert(epoch is not None)\n",
    "        step = (epoch + 1) * len(train_dataloader)\n",
    "        writer.add_scalar('Loss/Test', test_loss, step)\n",
    "        writer.add_scalar('Accuracy/Test', test_acc, step)\n",
    "\n",
    "    epoch_str = '' if epoch is None else '[Epoch {}/{}]'.format(\n",
    "            str(epoch).zfill(len(str(args.n_epochs))), args.n_epochs)\n",
    "\n",
    "    log = epoch_str + ' '\n",
    "    log += 'Train Loss: {:f}, '.format(train_loss)\n",
    "    log += 'Train Acc: {:.2f}%, '.format(train_acc)\n",
    "    log += 'Test Loss: {:f}, '.format(test_loss)\n",
    "    log += 'Test Acc: {:.2f}%.'.format(test_acc)\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': False, 'batch_size': 32, 'n_epochs': 50, 'n_workers': 4, 'learning_rate': 0.001, 'beta1': 0.9, 'beta2': 0.999, 'step_size': 20, 'gamma': 0.5, 'in_data_file': 'data/ModelNet/modelnet_classification.h5', 'model': 'outputs/model_50.pth', 'out_dir': 'outputs'}\n",
      "# classes: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Loss: 0.079747, Train Acc: 97.13%, Test Loss: 0.418972, Test Acc: 88.45%.\n"
     ]
    }
   ],
   "source": [
    "# Main function.\n",
    "if __name__ == \"__main__\":\n",
    "    print(args)\n",
    "\n",
    "    # Load datasets.\n",
    "    train_dataset, train_dataloader, test_dataset, test_dataloader, \\\n",
    "        n_classes = create_datasets_and_dataloaders()\n",
    "\n",
    "    # Create the network.\n",
    "    n_dims = 3\n",
    "    net = PointNetCls(n_dims, n_classes)\n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "\n",
    "    # Load a model if given.\n",
    "    if args.model != '':\n",
    "        net.load_state_dict(torch.load(args.model))\n",
    "\n",
    "    # Set an optimizer and a scheduler.\n",
    "    optimizer = torch.optim.Adam(\n",
    "        net.parameters(), lr=args.learning_rate,\n",
    "        betas=(args.beta1, args.beta2))\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "\n",
    "    # Create the output directory.\n",
    "    if not os.path.exists(args.out_dir):\n",
    "        os.makedirs(args.out_dir)\n",
    "\n",
    "    # Train.\n",
    "    if args.train:\n",
    "        writer = SummaryWriter(args.out_dir)\n",
    "\n",
    "        for epoch in range(args.n_epochs):\n",
    "            run_epoch_train_and_test(\n",
    "                train_dataset, train_dataloader, test_dataset, test_dataloader,\n",
    "                epoch, writer)\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                # Save the model.\n",
    "                model_file = os.path.join(\n",
    "                    args.out_dir, 'model_{:d}.pth'.format(epoch + 1))\n",
    "                torch.save(net.state_dict(), model_file)\n",
    "                print(\"Saved '{}'.\".format(model_file))\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        writer.close()\n",
    "    else:\n",
    "        run_epoch_train_and_test(\n",
    "            train_dataset, train_dataloader, test_dataset, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
